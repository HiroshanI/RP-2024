{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":1956405,"sourceType":"datasetVersion","datasetId":1060121},{"sourceId":7563141,"sourceType":"datasetVersion","datasetId":4403839},{"sourceId":7917880,"sourceType":"datasetVersion","datasetId":4652490},{"sourceId":8122196,"sourceType":"datasetVersion","datasetId":4799443}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from sklearn.metrics import f1_score, precision_score, recall_score\n# import torch\n# import torch.nn as nn\n# import pandas as pd\n# import numpy as np\n# from torch.utils.data import Dataset, DataLoader\n# import torch.nn.functional as F\n# from transformers import AutoTokenizer, AutoModel\n# from sklearn.model_selection import train_test_split\n# import random\n# from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n# from torch.nn.parallel import DataParallel\n# import matplotlib.pyplot as plt\n# import seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(model.state_dict(), 'model_weights.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_loaded = AutoModel.from_pretrained(\"bert-base-uncased\")\n# model_loaded = EmotionClassifier(model_loaded, num_classes)\n# model_loaded = DataParallel(model_loaded)\n# print(model_loaded)\n# model_loaded.load_state_dict(torch.load('model_weights.pth'))\n# model_loaded.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_encodings = tokenizer(\"I am done with you\", truncation=True, \n#                             padding=True, max_length=128, \n#                             return_tensors='pt')\n\n# inputs = {'input_ids': input_encodings['input_ids'].to(device),\n#           'attention_mask': input_encodings['attention_mask'].to(device)}\n# inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_loaded.eval()\n# with torch.no_grad():Z\n#     outputs = model_loaded(**inputs)\n# logits = outputs[0]\n# probabilities = torch.nn.functional.softmax(logits, dim=-1)\n# predicted_label_index = torch.argmax(probabilities)\n# predicted_label_index.item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5)","metadata":{}},{"cell_type":"markdown","source":"# Text Augmentation","metadata":{}},{"cell_type":"code","source":"!pip -qq install nltk==3.2.4\n!pip -qq install spacy_udpipe==1.0.0\n!pip -qq install tqdm==4.66.1\n!pip -qq install gensim==4.3.2","metadata":{"execution":{"iopub.status.busy":"2024-04-19T04:05:43.118174Z","iopub.execute_input":"2024-04-19T04:05:43.118547Z","iopub.status.idle":"2024-04-19T04:06:12.662549Z","shell.execute_reply.started":"2024-04-19T04:05:43.118517Z","shell.execute_reply":"2024-04-19T04:06:12.661473Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nnltk.download('stopwords')\nimport random\nimport gensim\nimport spacy_udpipe\nimport re\nfrom tqdm.notebook import tqdm_notebook as tqdm","metadata":{"execution":{"iopub.status.busy":"2024-04-19T04:06:23.479115Z","iopub.execute_input":"2024-04-19T04:06:23.479451Z","iopub.status.idle":"2024-04-19T04:06:28.490131Z","shell.execute_reply.started":"2024-04-19T04:06:23.479420Z","shell.execute_reply":"2024-04-19T04:06:28.489078Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy_udpipe\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/gensim/matutils.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.10/site-packages/scipy/linalg/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.10/site-packages/scipy/linalg/__init__.py)","output_type":"error"}]},{"cell_type":"markdown","source":"## Random swap","metadata":{}},{"cell_type":"code","source":"def random_swap(words, n):\n    new_words = words.copy()\n    for _ in range(n):\n        new_words = swap_word(new_words)\n    return new_words\n\ndef swap_word(new_words):\n    random_idx_1 = random.randint(0, len(new_words) - 1)\n    random_idx_2 = random_idx_1\n    counter = 0\n    while random_idx_2 == random_idx_1:\n        random_idx_2 = random.randint(0, len(new_words) - 1)\n        counter += 1\n        if counter > 3:\n            return new_words\n    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n    return new_words","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:28:59.519940Z","iopub.execute_input":"2024-04-18T17:28:59.520784Z","iopub.status.idle":"2024-04-18T17:28:59.527729Z","shell.execute_reply.started":"2024-04-18T17:28:59.520756Z","shell.execute_reply":"2024-04-18T17:28:59.526691Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Random insertion","metadata":{}},{"cell_type":"code","source":"def random_insertion(words, n):\n    new_words = words.copy()\n    for _ in range(n):\n            add_word(new_words)\n    return new_words\n\ndef add_word(new_words):\n    synonyms = []\n    counter = 0\n\n    while len(synonyms) <1:\n        random_word = new_words[random.randint(0, len(new_words)-1)]\n        #synonyms = self.synonyms_cadidates(random_word, self.df)\n        synonyms = list(get_synonyms_vec(random_word))\n        counter += 1\n        if counter > 10:\n            return\n\n    random_synonym = synonyms[0]\n    random_idx = random.randint(0, len(new_words)-1)\n    new_words.insert(random_idx, random_synonym)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:28:59.528955Z","iopub.execute_input":"2024-04-18T17:28:59.529260Z","iopub.status.idle":"2024-04-18T17:28:59.538244Z","shell.execute_reply.started":"2024-04-18T17:28:59.529235Z","shell.execute_reply":"2024-04-18T17:28:59.537495Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Random deletion","metadata":{}},{"cell_type":"code","source":"def random_deletion(words, p):\n    \"\"\"\n    Randomly delete words from a sentence with probability p\n    :param words:\n    :param p:\n    :return:\n    \"\"\"\n    #obviously, if there's only one word, don't delete it\n    if len(words) == 1:\n        return words\n\n    #randomly delete words with probability p\n    new_words = []\n\n    for word in words:\n        r = random.uniform(0, 1) # random number between 0.0 and 1.0\n        if r > p: #kinda elegant when you think about it\n            new_words.append(word)\n\n    #if you end up deleting all words, just return a random word\n    if len(new_words) == 0:\n        rand_int = random.randint(0, len(words)-1)\n        return [words[rand_int]]\n\n    return new_words","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:28:59.541886Z","iopub.execute_input":"2024-04-18T17:28:59.542936Z","iopub.status.idle":"2024-04-18T17:28:59.549134Z","shell.execute_reply.started":"2024-04-18T17:28:59.542910Z","shell.execute_reply":"2024-04-18T17:28:59.548387Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Random Replacement (Vector-based)","metadata":{}},{"cell_type":"code","source":"stop_words = list(set(nltk.corpus.stopwords.words('english')))\ndef synonym_replacement_vec(words, n):\n    \n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if word not in stop_words]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    for random_word in random_word_list:\n        synonyms = get_synonyms_vec(random_word)\n        if len(synonyms) >= 1:\n            synonym = random.choice(list(synonyms))\n            new_words = [synonym if word.lower() == random_word else word for word in new_words]\n            # print(\"replaced\", random_word, \"with\", synonym)\n            num_replaced += 1\n        if num_replaced >= n:  # only replace up to n words\n            break\n\n    # this is stupid but we need it, trust me\n    sentence = ' '.join(new_words)\n    new_words = sentence.split(' ')\n    \n    return new_words\n\n############################################\n\ndef get_synonyms_vec(word):\n    \n    synonyms = set()\n    flag = False\n    vec = None\n    try:\n        vec = wv_from_text.similar_by_word(word.lower())\n    except KeyError:\n        flag = True\n        pass\n\n    if flag is False:\n        synonyms.add(vec[0][0])\n\n    if word in synonyms:\n        synonyms.remove(word)\n\n    return synonyms","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:30:06.326175Z","iopub.execute_input":"2024-04-18T17:30:06.327057Z","iopub.status.idle":"2024-04-18T17:30:06.335883Z","shell.execute_reply.started":"2024-04-18T17:30:06.327022Z","shell.execute_reply":"2024-04-18T17:30:06.334760Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Complete DDA","metadata":{}},{"cell_type":"code","source":"def augmentation(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, alpha_rd=0.1, num_aug=9):\n    \"\"\"\n    @param sentence\n    @param alpha_sr synonym replacement rate, percentage of the total sentence\n    @param alpha_ri random insertion rate, percentage of the total sentence\n    @param alpha_rs random swap rate, percentage of the total sentence\n    @param alpha_rd random deletion rate, percentage of the total sentence\n    @param num_aug how many augmented sentences to create\n\n    @return list of augmented sentences\n    \"\"\"\n    words_list = sentence.split(' ')  # list of words in the sentence\n    words = [word for word in words_list if word != '']  # remove empty words\n    num_words = len(words_list)  # number of words in the sentence\n\n    augmented_sentences = []\n    num_new_per_technique = int(num_aug / 4) + 1 # number of augmented sentences per technique\n \n    #synonmym replacement\n    if (alpha_sr > 0):\n        n_sr = max(1, int(alpha_sr * num_words)) # number of words to be replaced per technique\n        #print(\"Number of words to be replaced per technique: \", n_sr)\n        for _ in range(num_new_per_technique):\n            a_words = synonym_replacement_vec(words, n_sr)\n            augmented_sentences.append(' '.join(a_words))\n    #random insertion\n    if (alpha_ri > 0):\n        n_ri = max(1,int(alpha_ri * num_words))\n        for _ in range(num_new_per_technique):\n            a_words = random_insertion(words, n_ri)\n            augmented_sentences.append(' '.join(a_words))\n    #Random Deletion\n    if (alpha_rd > 0):\n        for _ in range(num_new_per_technique):\n            a_words = random_deletion(words, alpha_rd)\n            augmented_sentences.append(' '.join(a_words))\n    #Random Swap\n    if (alpha_rs > 0):\n        n_rs = max(1, int(alpha_rs * num_words))\n        for _ in range(num_new_per_technique):\n            a_words = random_swap(words, n_rs)\n            augmented_sentences.append(' '.join(a_words))\n\n    return augmented_sentences","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:30:07.810790Z","iopub.execute_input":"2024-04-18T17:30:07.811584Z","iopub.status.idle":"2024-04-18T17:30:07.820470Z","shell.execute_reply.started":"2024-04-18T17:30:07.811553Z","shell.execute_reply":"2024-04-18T17:30:07.819472Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def create_encodings(df):\n    texts = [gensim.utils.simple_preprocess(text) for text in df.text]\n    words = [word.encode('utf-8') for words in texts for word in words]\n    # Training Word2Vec model\n    w2v_model = gensim.models.Word2Vec(texts, min_count=1, vector_size=300)\n    # Save Model \n    w2v_model.wv.save_word2vec_format('word2vec_goemotions_300dim.txt', binary=False)\n    \ndef create_augmented_df(df, sr, ri, rs, rd, n):\n    aug_data = {0:[],1:[],2:[],3:[],4:[],5:[],6:[]}\n    n_sentences = df.shape[0]\n    tqdm.pandas(desc=\"Augmentation Progress \")\n    df['augmented'] = df.text.progress_apply(lambda x: augmentation(x, \n                                                           alpha_sr=0.3, alpha_ri=0.2, \n                                                           alpha_rs=0.4, alpha_rd=0.3, \n                                                           num_aug=4))\n    aug_df = df[['augmented','label']].rename(columns={'augmented':'text'})\n    aug_df = aug_df.explode('text', ignore_index=True)\n    aug_df.to_csv(f\"goemotions_aug_{sr}_{ri}_{rs}_{rd}_{n}.csv\")\n    return aug_df\n\ndef sample_df(df, n_rows=None):\n    n_rows = df.label.value_counts().min() if n_rows == None else n_rows\n    def sample_rows(group, x):\n        if group.shape[0] > x:\n            return group.sample(x)\n        else:\n            return group\n    \n    sampled_df = df.groupby('label')[['label','text']].apply(lambda group: sample_rows(group, n_rows))\n    return sampled_df","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:30:08.696304Z","iopub.execute_input":"2024-04-18T17:30:08.696680Z","iopub.status.idle":"2024-04-18T17:30:08.706789Z","shell.execute_reply.started":"2024-04-18T17:30:08.696653Z","shell.execute_reply":"2024-04-18T17:30:08.705805Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/goemotions-7-emotions/goemotions.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:30:09.711147Z","iopub.execute_input":"2024-04-18T17:30:09.711497Z","iopub.status.idle":"2024-04-18T17:30:09.716160Z","shell.execute_reply.started":"2024-04-18T17:30:09.711470Z","shell.execute_reply":"2024-04-18T17:30:09.715028Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(DATA_PATH)\nprint(f\"Original dataset: {data.shape}\")\ncreate_encodings(data) # Create Word2Vec embeddings\nwv_from_text = gensim.models.KeyedVectors.load_word2vec_format('/kaggle/working/word2vec_goemotions_300dim.txt', \n                                                               binary=False)\nsampled_data = sample_df(data)\nprint(f\"Sampled dataset: {sampled_data.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:30:32.981916Z","iopub.execute_input":"2024-04-18T17:30:32.982777Z","iopub.status.idle":"2024-04-18T17:30:50.251581Z","shell.execute_reply.started":"2024-04-18T17:30:32.982745Z","shell.execute_reply":"2024-04-18T17:30:50.250489Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Original dataset: (54263, 3)\nSampled dataset: (5166, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"data_augmented = create_augmented_df(sampled_data, .3, .2, .4, .3, 4)\nprint(f\"Augmented dataset: {data_augmented.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training BERT","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport torch\nfrom torch.nn.parallel import DataParallel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tqdm.notebook import tqdm_notebook\n\ndef create_dataloader(df, tokenizer):\n    train_X, val_X, train_y, val_y = train_test_split(df[\"text\"], \n                                                      df[\"label\"], \n                                                      test_size=0.2, \n                                                      random_state=42)\n    train_X = train_X.tolist()\n    val_X = val_X.tolist()\n    train_y = np.array(train_y, dtype=np.float64)\n    val_y = np.array(val_y, dtype=np.float64)\n    train_encodings = tokenizer(train_X, truncation=True, \n                                padding=True, max_length=128, \n                                return_tensors='pt')\n    val_encodings = tokenizer(val_X, truncation=True, \n                              padding=True, max_length=128, \n                              return_tensors='pt')\n    train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'],\n                                                  train_encodings['attention_mask'],\n                                                  torch.tensor(train_y, dtype=torch.float64))\n    val_dataset = torch.utils.data.TensorDataset(val_encodings['input_ids'],\n                                                val_encodings['attention_mask'],\n                                                torch.tensor(val_y, dtype=torch.float64))\n    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=32)\n    return train_dataloader, val_dataloader\n\nclass Classifier(torch.nn.Module):\n    def __init__(self, transformer_model, num_classes):\n        super(Classifier, self).__init__()\n        self.transformer = transformer_model\n        self.fc = torch.nn.Linear(768, num_classes)  \n        \n    def forward(self, input_ids, attention_mask):\n        output = self.transformer(input_ids=input_ids, \n                                  attention_mask=attention_mask)\n        pooled_output = output.pooler_output \n        logits = self.fc(pooled_output)\n        return logits\n    \ndef train_model(df):\n    \n    model = AutoModel.from_pretrained(\"bert-base-uncased\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    train_dataloader, val_dataloader = create_dataloader(df, tokenizer)\n    num_classes = 7\n    num_epochs = 3\n    learning_rate = 1e-5\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = Classifier(model, num_classes)\n    model = DataParallel(model)\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epoch in tqdm_notebook(range(num_epochs)):\n        model.train()\n        total_loss = 0.0\n        total_correct = 0\n\n        # Training loop\n        print(f\"Training Epoch {epoch+1}\")\n        for batch in tqdm_notebook(train_dataloader):\n            input_ids, attention_mask, labels = batch\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n\n            labels = labels.to(device).long()\n            outputs = outputs.float()\n\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total_correct += (predicted == labels).sum().item()\n\n#             if i  % 50 == 0:\n#                 print(f\"Batch {i:4d}  |  Loss {loss.item():.4f}\")\n\n        train_loss = total_loss / len(train_dataloader)\n        train_accuracy = total_correct / len(train_dataloader.dataset)\n\n        # Validation loop\n        model.eval()\n        total_val_loss = 0.0\n        total_val_correct = 0\n        val_predicted = []\n        val_labels = []\n\n        print(f\"Validation Epoch {epoch+1}\")\n        with torch.no_grad():\n            for batch in tqdm_notebook(val_dataloader):\n                input_ids, attention_mask, labels = batch\n                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n                outputs = model(input_ids, attention_mask)\n                labels = labels.to(device).long()\n\n                outputs = outputs.float()\n                loss = criterion(outputs, labels)\n\n                total_val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                total_val_correct += (predicted == labels).sum().item()\n                val_predicted.extend(predicted.cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n\n#                 if i % 50 == 0:\n#                     print(f\"Batch {i:4d}  |  Loss {loss.item():.4f}\")\n                    \n    val_predicted = np.array(val_predicted)\n    val_labels = np.array(val_labels)\n\n    print(classification_report(val_labels, val_predicted, target_names = [f'Class {i}' for i in range(num_classes)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_augmented = pd.read_csv(\"/kaggle/working/goemotions_aug_0.3_0.2_0.4_0.3_4.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(data_augmented)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}