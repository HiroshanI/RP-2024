{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7563141,"sourceType":"datasetVersion","datasetId":4403839},{"sourceId":8122196,"sourceType":"datasetVersion","datasetId":4799443}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip -q install nltk==3.2.4\n!pip -q install spacy_udpipe==1.0.0\n!pip -q install tqdm==4.66.1\n!pip -q install gensim==4.3.2","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:15:04.655216Z","iopub.execute_input":"2024-04-29T10:15:04.655568Z","iopub.status.idle":"2024-04-29T10:15:57.105795Z","shell.execute_reply.started":"2024-04-29T10:15:04.655539Z","shell.execute_reply":"2024-04-29T10:15:57.104656Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nnltk.download('stopwords')\nimport random\nimport gensim\nimport spacy_udpipe\nimport re\nfrom tqdm.notebook import tqdm_notebook as tqdm","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:15:57.107912Z","iopub.execute_input":"2024-04-29T10:15:57.108228Z","iopub.status.idle":"2024-04-29T10:16:17.398383Z","shell.execute_reply.started":"2024-04-29T10:15:57.108200Z","shell.execute_reply":"2024-04-29T10:16:17.397379Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Augmentation\n## DDA Functions","metadata":{}},{"cell_type":"code","source":"# RANDOM SWAP ---------------------------------------------------\ndef random_swap(words, n):\n    new_words = words.copy()\n    for _ in range(n):\n        new_words = swap_word(new_words)\n    return new_words\n\ndef swap_word(new_words):\n    random_idx_1 = random.randint(0, len(new_words) - 1)\n    random_idx_2 = random_idx_1\n    counter = 0\n    while random_idx_2 == random_idx_1:\n        random_idx_2 = random.randint(0, len(new_words) - 1)\n        counter += 1\n        if counter > 3:\n            return new_words\n    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n    return new_words\n\n# RANDOM INSERTION ---------------------------------------------------\ndef random_insertion(words, n):\n    new_words = words.copy()\n    for _ in range(n):\n            add_word(new_words)\n    return new_words\n\ndef add_word(new_words):\n    synonyms = []\n    counter = 0\n\n    while len(synonyms) <1:\n        random_word = new_words[random.randint(0, len(new_words)-1)]\n        #synonyms = self.synonyms_cadidates(random_word, self.df)\n        synonyms = list(get_synonyms_vec(random_word))\n        counter += 1\n        if counter > 10:\n            return\n\n    random_synonym = synonyms[0]\n    random_idx = random.randint(0, len(new_words)-1)\n    new_words.insert(random_idx, random_synonym)\n\n# RANDOM DELETION ---------------------------------------------------\ndef random_deletion(words, p):\n    \"\"\"\n    Randomly delete words from a sentence with probability p\n    :param words:\n    :param p:\n    :return:\n    \"\"\"\n    if len(words) == 1:\n        return words\n    new_words = []\n    for word in words:\n        r = random.uniform(0, 1) # random number between 0.0 and 1.0\n        if r > p: #kinda elegant when you think about it\n            new_words.append(word)\n    #if you end up deleting all words, just return a random word\n    if len(new_words) == 0:\n        rand_int = random.randint(0, len(words)-1)\n        return [words[rand_int]]\n\n    return new_words\n\n# VECTOR-BASED SYNONYM REPLACEMENT ----------------------------------\nstop_words = list(set(nltk.corpus.stopwords.words('english')))\ndef synonym_replacement_vec(words, n):\n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if word not in stop_words]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    for random_word in random_word_list:\n        synonyms = get_synonyms_vec(random_word)\n        if len(synonyms) >= 1:\n            synonym = random.choice(list(synonyms))\n            new_words = [synonym if word.lower() == random_word else word for word in new_words]\n            # print(\"replaced\", random_word, \"with\", synonym)\n            num_replaced += 1\n        if num_replaced >= n:  # only replace up to n words\n            break\n    # this is stupid but we need it, trust me\n    sentence = ' '.join(new_words)\n    new_words = sentence.split(' ')\n    return new_words\n\ndef get_synonyms_vec(word):\n    synonyms = set()\n    flag = False\n    vec = None\n    try:\n        vec = wv_from_text.similar_by_word(word.lower())\n    except KeyError:\n        flag = True\n        pass\n    if flag is False:\n        synonyms.add(vec[0][0])\n    if word in synonyms:\n        synonyms.remove(word)\n    return synonyms","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:16:17.399610Z","iopub.execute_input":"2024-04-29T10:16:17.400319Z","iopub.status.idle":"2024-04-29T10:16:17.423679Z","shell.execute_reply.started":"2024-04-29T10:16:17.400281Z","shell.execute_reply":"2024-04-29T10:16:17.422879Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def augmentation(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, alpha_rd=0.1, num_aug=9):\n    \"\"\"\n    @param sentence\n    @param alpha_sr synonym replacement rate, percentage of the total sentence\n    @param alpha_ri random insertion rate, percentage of the total sentence\n    @param alpha_rs random swap rate, percentage of the total sentence\n    @param alpha_rd random deletion rate, percentage of the total sentence\n    @param num_aug how many augmented sentences to create\n\n    @return list of augmented sentences\n    \"\"\"\n    words_list = sentence.split(' ')  # list of words in the sentence\n    words = [word for word in words_list if word != '']  # remove empty words\n    num_words = len(words_list)  # number of words in the sentence\n\n    augmented_sentences = []\n    num_new_per_technique = int(num_aug / 4) + 1 # number of augmented sentences per technique\n \n    #synonmym replacement\n    if (alpha_sr > 0):\n        n_sr = max(1, int(alpha_sr * num_words)) # number of words to be replaced per technique\n        #print(\"Number of words to be replaced per technique: \", n_sr)\n        for _ in range(num_new_per_technique):\n            a_words = synonym_replacement_vec(words, n_sr)\n            augmented_sentences.append(' '.join(a_words))\n    #random insertion\n    if (alpha_ri > 0):\n        n_ri = max(1,int(alpha_ri * num_words))\n        for _ in range(num_new_per_technique):\n            a_words = random_insertion(words, n_ri)\n            augmented_sentences.append(' '.join(a_words))\n    #Random Deletion\n    if (alpha_rd > 0):\n        for _ in range(num_new_per_technique):\n            a_words = random_deletion(words, alpha_rd)\n            augmented_sentences.append(' '.join(a_words))\n    #Random Swap\n    if (alpha_rs > 0):\n        n_rs = max(1, int(alpha_rs * num_words))\n        for _ in range(num_new_per_technique):\n            a_words = random_swap(words, n_rs)\n            augmented_sentences.append(' '.join(a_words))\n\n    return augmented_sentences","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:16:17.425737Z","iopub.execute_input":"2024-04-29T10:16:17.426125Z","iopub.status.idle":"2024-04-29T10:16:17.436868Z","shell.execute_reply.started":"2024-04-29T10:16:17.426100Z","shell.execute_reply":"2024-04-29T10:16:17.435989Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def create_encodings(df, name):\n    texts = [gensim.utils.simple_preprocess(text) for text in df.text]\n    words = [word.encode('utf-8') for words in texts for word in words]\n    w2v_model = gensim.models.Word2Vec(texts, min_count=1, vector_size=300) # Train Word2Vec model\n    filename = f'word2vec_{name}_300dim.txt'\n    w2v_model.wv.save_word2vec_format(filename, binary=False) # Save Model \n    return filename\n    \ndef create_augmented_df(df, val_df, sr, ri, rs, rd, n, name):\n    aug_data = {0:[],1:[],2:[],3:[],4:[],5:[],6:[]}\n    n_sentences = df.shape[0]\n    tqdm.pandas(desc=\"Augmentation Progress \")\n    df['augmented'] = df.text.progress_apply(lambda x: augmentation(x, \n                                                           alpha_sr=0.3, alpha_ri=0.2, \n                                                           alpha_rs=0.4, alpha_rd=0.3, \n                                                           num_aug=4))\n    aug_df = df[['augmented','label']].rename(columns={'augmented':'text'})\n    aug_df = aug_df.explode('text', ignore_index=True)\n    aug_df.to_csv(f\"{name}_aug_{sr}_{ri}_{rs}_{rd}_{n}.csv\")\n    val_df.to_csv(f\"{name}_aug_{sr}_{ri}_{rs}_{rd}_{n}_test.csv\")\n    return aug_df\n\ndef sample_df(df, n_rows=None):\n    n_rows = df.label.value_counts().min() if n_rows == None else n_rows\n    def sample_rows(group, x):\n        if group.shape[0] > x:\n            return group.sample(x)\n        else:\n            return group\n    sampled_df = df.groupby('label')[['label','text']].apply(lambda group: sample_rows(group, n_rows)).reset_index(drop=True)\n    sampled_df = sampled_df.sample(frac=1).reset_index(drop=True)\n    return sampled_df","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:16:17.437917Z","iopub.execute_input":"2024-04-29T10:16:17.438222Z","iopub.status.idle":"2024-04-29T10:16:17.452054Z","shell.execute_reply.started":"2024-04-29T10:16:17.438198Z","shell.execute_reply":"2024-04-29T10:16:17.451225Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{}},{"cell_type":"code","source":"# DATA_PATH = \"/kaggle/input/goemotions-7-emotions/goemotions.csv\"\nDATA_PATH = \"/kaggle/input/emotions/text.csv\"\n\ndef get_filename(path):\n    return path.split('/')[-1].split('.')[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:59:23.191460Z","iopub.execute_input":"2024-04-29T10:59:23.192366Z","iopub.status.idle":"2024-04-29T10:59:23.196973Z","shell.execute_reply.started":"2024-04-29T10:59:23.192333Z","shell.execute_reply":"2024-04-29T10:59:23.196025Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv(DATA_PATH)\nprint(f\"Original dataset: {data.shape}\")\n\nsampled_data = sample_df(data, 1000)\nsampled_data.to_csv(f'{get_filename(DATA_PATH)}_sampled_{sampled_data.shape[0]}.csv')\nprint(f\"Sampled dataset: {sampled_data.shape}\")\nenc_filename = create_encodings(data, get_filename(DATA_PATH)) # Create Word2Vec embeddings\nwv_from_text = gensim.models.KeyedVectors.load_word2vec_format(enc_filename, \n                                                               binary=False)\nprint(\"Word2Vec loaded from:\", enc_filename)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:02:10.576311Z","iopub.execute_input":"2024-04-29T11:02:10.577325Z","iopub.status.idle":"2024-04-29T11:03:58.242196Z","shell.execute_reply.started":"2024-04-29T11:02:10.577289Z","shell.execute_reply":"2024-04-29T11:03:58.241215Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Original dataset: (416809, 3)\nSampled dataset: (6000, 2)\nWord2Vec loaded from: word2vec_text_300dim.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = list(sampled_data.label)\ntrain_X, val_X, train_y, val_y = train_test_split(sampled_data.text, \n                                                  sampled_data.label, \n                                                  test_size=0.2, random_state=42, stratify=y)\n\ntrain_data = pd.DataFrame({'text':train_X, 'label':train_y})\nval_data = pd.DataFrame({'text':val_X, 'label':val_y}).reset_index()\n\nprint(f\"Training data: {train_data.shape}\")\n\ntrain_data_augmented = create_augmented_df(train_data, val_data,\n                                        .3, .2, .4, .3, , \n                                        name=get_filename(DATA_PATH))\nprint(f\"Augmented Training data: {train_data_augmented.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:04:14.121909Z","iopub.execute_input":"2024-04-29T11:04:14.122654Z","iopub.status.idle":"2024-04-29T11:11:32.093886Z","shell.execute_reply.started":"2024-04-29T11:04:14.122620Z","shell.execute_reply":"2024-04-29T11:11:32.092963Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Original data: (4800, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Augmentation Progress :   0%|          | 0/4800 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b872b328b7744b719d9d579ee659d260"}},"metadata":{}},{"name":"stdout","text":"Augmented data: (38400, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tuning BERT","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.nn.parallel import DataParallel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\ndef split_data(df):\n    y = list(df.label)\n    train_X, val_X, train_y, val_y = train_test_split(df.text, df.label, \n                                                      test_size=0.2, random_state=42, \n                                                      stratify=y)\n    return train_X, val_X, train_y, val_y\n\ndef create_dataloader(tokenizer, \n                      train_X=None, val_X=None, \n                      train_y=None, val_y=None):\n    train_X = train_X.tolist()\n    val_X = val_X.tolist()\n    train_y = np.array(train_y, dtype=np.float64)\n    val_y = np.array(val_y, dtype=np.float64)\n    train_encodings = tokenizer(train_X, truncation=True, \n                                padding=True, max_length=128, \n                                return_tensors='pt')\n    val_encodings = tokenizer(val_X, truncation=True, \n                              padding=True, max_length=128, \n                              return_tensors='pt')\n    train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'],\n                                                  train_encodings['attention_mask'],\n                                                  torch.tensor(train_y, dtype=torch.float64))\n    val_dataset = torch.utils.data.TensorDataset(val_encodings['input_ids'],\n                                                val_encodings['attention_mask'],\n                                                torch.tensor(val_y, dtype=torch.float64))\n    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=32)\n    return train_dataloader, val_dataloader\n\nclass Classifier(torch.nn.Module):\n    def __init__(self, transformer_model, num_classes):\n        super(Classifier, self).__init__()\n        self.transformer = transformer_model\n        self.fc = torch.nn.Linear(768, num_classes)  \n        \n    def forward(self, input_ids, attention_mask):\n        output = self.transformer(input_ids=input_ids, \n                                  attention_mask=attention_mask)\n        pooled_output = output.pooler_output \n        logits = self.fc(pooled_output)\n        return logits\n    \ndef download_models():\n    model = AutoModel.from_pretrained(\"bert-base-uncased\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    return model, tokenizer\nmodel, tokenizer = download_models()\n    \ndef train_model(train_X, val_X, train_y, val_y, model=model, tokenizer=tokenizer):\n    train_dataloader, val_dataloader = create_dataloader(tokenizer, train_X, val_X, train_y, val_y)\n    num_classes = 6\n    num_epochs = 3\n    learning_rate = 1e-5\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = Classifier(model, num_classes)\n    model = DataParallel(model)\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epoch in tqdm(range(num_epochs), desc=\"Epochs Progress \"):\n        model.train()\n        total_loss = 0.0\n        total_correct = 0\n\n        # Training loop \n        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1} \"):\n            input_ids, attention_mask, labels = batch\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n\n            labels = labels.to(device).long()\n            outputs = outputs.float()\n\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total_correct += (predicted == labels).sum().item()\n\n#             if i  % 100 == 0:\n#                 print(f\"Loss: {loss:.7f}  |  Batch: [{i:>5d}/{len(train_dataloader):>5d}]\")\n\n        train_loss = total_loss / len(train_dataloader)\n        train_accuracy = total_correct / len(train_dataloader.dataset)\n\n    # Validation loop\n    model.eval()\n    total_val_loss = 0.0\n    total_val_correct = 0\n    val_predicted = []\n    val_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_dataloader, desc=f\"Validation \"):\n            input_ids, attention_mask, labels = batch\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n            outputs = model(input_ids, attention_mask)\n            labels = labels.to(device).long()\n\n            outputs = outputs.float()\n            loss = criterion(outputs, labels)\n\n            total_val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total_val_correct += (predicted == labels).sum().item()\n            val_predicted.extend(predicted.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n#                 if i % 50 == 0:\n#                     print(f\"Batch {i:4d}  |  Loss {loss.item():.4f}\")\n                    \n    val_predicted = np.array(val_predicted)\n    val_labels = np.array(val_labels)\n\n    print(classification_report(val_labels, val_predicted, target_names = [f'Class {i}' for i in range(num_classes)]))","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:26:19.242100Z","iopub.execute_input":"2024-04-29T11:26:19.243106Z","iopub.status.idle":"2024-04-29T11:26:19.639541Z","shell.execute_reply.started":"2024-04-29T11:26:19.243062Z","shell.execute_reply":"2024-04-29T11:26:19.638767Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"data_ge_augmented = pd.read_csv(\"goemotions_aug_0.3_0.2_0.4_0.3_4.csv\")\ndata_ge_augmented_test = pd.read_csv(\"goemotions_aug_0.3_0.2_0.4_0.3_4_test.csv\")\ndata_ge_sampled = pd.read_csv(\"goemotions_sampled_5166.csv\")\ndata_ge_original = pd.read_csv(\"/kaggle/input/goemotions-7-emotions/goemotions.csv\")\ndata_e_sampled = pd.read_csv(\"text_sampled_6000.csv\")\ndata_e_augmented = pd.read_csv(\"text_aug_0.3_0.2_0.4_0.3_4.csv\")\ndata_e_augmented_test = pd.read_csv(\"text_aug_0.3_0.2_0.4_0.3_4_test.csv\")\n\nprint(\"GOEMOTIONS -------------------------------\",\n      f\"Augmented Train Data : {data_ge_augmented.shape}\",\n      f\"Sampled Test Data : {data_ge_augmented_test.shape}\",\n      f\"Sampled Original Data : {data_ge_sampled.shape}\",\n      f\"Original Data : {data_ge_original.shape}\", \n      \"EMOTIONS -------------------------------\",\n      f\"Augmented Train Data : {data_e_augmented.shape}\",\n      f\"Sampled Test Data : {data_e_augmented_test.shape}\",\n      f\"Sampled Original Data : {data_e_sampled.shape}\",\n      sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:12:42.796217Z","iopub.execute_input":"2024-04-29T11:12:42.796601Z","iopub.status.idle":"2024-04-29T11:12:43.033849Z","shell.execute_reply.started":"2024-04-29T11:12:42.796570Z","shell.execute_reply":"2024-04-29T11:12:43.032884Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"GOEMOTIONS -------------------------------\nAugmented Train Data : (33056, 3)\nSampled Test Data : (1034, 4)\nSampled Original Data : (5166, 3)\nOriginal Data : (54263, 3)\nEMOTIONS -------------------------------\nAugmented Train Data : (38400, 3)\nSampled Test Data : (1200, 4)\nSampled Original Data : (6000, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# splitted_data = split_data(data_ge_sampled)\nsplitted_data = split_data(data_e_sampled)\ntrain_model(*splitted_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:26:22.110902Z","iopub.execute_input":"2024-04-29T11:26:22.111792Z","iopub.status.idle":"2024-04-29T11:28:43.954655Z","shell.execute_reply.started":"2024-04-29T11:26:22.111756Z","shell.execute_reply":"2024-04-29T11:28:43.953718Z"},"trusted":true},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epochs Progress :   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d93c184725466abcecebc5457c40d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Epoch 1 :   0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7793cfde5303455a98dc17c759584651"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Epoch 2 :   0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f23180fedba4cdb9ddee741044501da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Epoch 3 :   0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39847b5765dc4bf8a5b5508f77477e82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation :   0%|          | 0/38 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50de4214ab75415bb5ed422825f05bbf"}},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n     Class 0       0.85      0.91      0.88       200\n     Class 1       0.90      0.82      0.86       200\n     Class 2       0.90      0.90      0.90       200\n     Class 3       0.90      0.94      0.92       200\n     Class 4       0.91      0.85      0.88       200\n     Class 5       0.91      0.96      0.94       200\n\n    accuracy                           0.90      1200\n   macro avg       0.90      0.90      0.90      1200\nweighted avg       0.90      0.90      0.90      1200\n\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n#### GoEmomtions Dataset ####\n# train_model(data_ge_augmented['text'], data_ge_augmented_test['text'],\n#            data_ge_augmented['label'], data_ge_augmented_test['label'])\n\n#### Emotions Dataset ####\ntrain_model(data_e_augmented['text'], data_e_augmented_test['text'],\n           data_e_augmented['label'], data_e_augmented_test['label'])","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:29:22.751656Z","iopub.execute_input":"2024-04-29T11:29:22.752036Z","iopub.status.idle":"2024-04-29T11:49:59.833880Z","shell.execute_reply.started":"2024-04-29T11:29:22.752006Z","shell.execute_reply":"2024-04-29T11:49:59.833064Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epochs Progress :   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c73cb45a7e24cd9a1a6375f3eb341fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Epoch 1 :   0%|          | 0/1200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"438c124933a14066b2c5b71bc623d700"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Epoch 2 :   0%|          | 0/1200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3962338bdcc4876a56f2f93e69bcc22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Epoch 3 :   0%|          | 0/1200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cecc1415f58b470783a71c0eb4fb78a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation :   0%|          | 0/38 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df097eefe97f44c4b1439cb5839434f2"}},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n     Class 0       0.83      0.82      0.83       200\n     Class 1       0.92      0.80      0.86       200\n     Class 2       0.84      0.95      0.89       200\n     Class 3       0.87      0.86      0.87       200\n     Class 4       0.84      0.81      0.83       200\n     Class 5       0.88      0.93      0.91       200\n\n    accuracy                           0.86      1200\n   macro avg       0.86      0.86      0.86      1200\nweighted avg       0.86      0.86      0.86      1200\n\n","output_type":"stream"}]}]}